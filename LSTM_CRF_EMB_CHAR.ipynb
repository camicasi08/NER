{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_CRF_EMB_CHAR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DlcSBTbvPIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "sys.path.append('./drive/My Drive/Colab Notebooks/Modules')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJ8J_JPxsZgm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "da4575ea-3b00-4c8b-eb25-885649357ada",
        "tags": []
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from preprocessing import get_vocab, index_sents\n",
        "from embedding import create_embeddings\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Using TensorFlow backend.\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3qbk3bYwlcJ",
        "colab_type": "code",
        "colab": {},
        "tags": []
      },
      "source": [
        "!pip install keras-tqdm\n",
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Model\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers import concatenate, Input, LSTM, Dropout, Embedding\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.utils import save_load_utils\n",
        "from gensim.models import Word2Vec\n",
        "from keras_tqdm import TQDMNotebookCallback\n",
        "from embedding import load_vocab"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: keras-tqdm in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (2.0.1)\nRequirement already satisfied: tqdm in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras-tqdm) (4.36.1)\nRequirement already satisfied: Keras in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras-tqdm) (2.3.1)\nRequirement already satisfied: numpy>=1.9.1 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (1.16.5)\nRequirement already satisfied: scipy>=0.14 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (1.3.1)\nRequirement already satisfied: six>=1.9.0 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (1.12.0)\nRequirement already satisfied: pyyaml in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (5.1.2)\nRequirement already satisfied: h5py in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (2.9.0)\nRequirement already satisfied: keras_applications>=1.0.6 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (1.0.8)\nRequirement already satisfied: keras_preprocessing>=1.0.5 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from Keras->keras-tqdm) (1.1.0)\nCollecting git+https://www.github.com/keras-team/keras-contrib.git\n  Cloning https://www.github.com/keras-team/keras-contrib.git to c:\\users\\camilo\\appdata\\local\\temp\\pip-req-build-vnmduso2\nRequirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages\\keras_contrib-2.0.8-py3.7.egg\nRequirement already satisfied: keras in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras-contrib==2.0.8) (2.3.1)\nRequirement already satisfied: numpy>=1.9.1 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.16.5)\nRequirement already satisfied: scipy>=0.14 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.3.1)\nRequirement already satisfied: six>=1.9.0 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.12.0)\nRequirement already satisfied: pyyaml in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (5.1.2)\nRequirement already satisfied: h5py in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (2.9.0)\nRequirement already satisfied: keras_applications>=1.0.6 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.0.8)\nRequirement already satisfied: keras_preprocessing>=1.0.5 in c:\\users\\camilo\\anaconda3\\envs\\thesis\\lib\\site-packages (from keras->keras-contrib==2.0.8) (1.1.0)\nBuilding wheels for collected packages: keras-contrib\n  Building wheel for keras-contrib (setup.py): started\n  Building wheel for keras-contrib (setup.py): finished with status 'done'\n  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp37-none-any.whl size=101658 sha256=f8588f4e2437e05c8352ef29cd22ff7a787ceda18147265b65be66dd77d0fefb\n  Stored in directory: C:\\Users\\Camilo\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-navorxne\\wheels\\11\\27\\c8\\4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\nSuccessfully built keras-contrib\n  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git 'C:\\Users\\Camilo\\AppData\\Local\\Temp\\pip-req-build-vnmduso2'\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgcZfj3Tt1E6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('./corpus/ancora/ancora_corpus_pos.csv')\n",
        "del data['Unnamed: 0']"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs989Hq0uCD9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "9497b829-9bd9-4b48-8790-28259db8e50e"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   Sentence #      Word Tag    POS\n0           0       Las   O    DET\n1           0  reservas   O   NOUN\n2           0        de   O    ADP\n3           0       oro   O   NOUN\n4           0         y   O  CCONJ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sentence #</th>\n      <th>Word</th>\n      <th>Tag</th>\n      <th>POS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>Las</td>\n      <td>O</td>\n      <td>DET</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>reservas</td>\n      <td>O</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>de</td>\n      <td>O</td>\n      <td>ADP</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>oro</td>\n      <td>O</td>\n      <td>NOUN</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>y</td>\n      <td>O</td>\n      <td>CCONJ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgXVXRLIxAU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentmarks = data[\"Sentence #\"].unique().tolist()\n",
        "words = data[\"Word\"].tolist()\n",
        "postags = data[\"POS\"].tolist()\n",
        "nertags = data[\"Tag\"].tolist()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VudepyCWxDfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "30c21101-88d4-4272-f916-77c34fc842c0",
        "tags": []
      },
      "source": [
        "%%time\n",
        "sentence_text = []\n",
        "sentence_post = []\n",
        "sentence_ners = []\n",
        "\n",
        "vocab = []\n",
        "\n",
        "this_snt = []\n",
        "this_pos = []\n",
        "this_ner = []\n",
        "#print(sentmarks[:10])\n",
        "for idx, s in enumerate(sentmarks):\n",
        "    #print(s)\n",
        "    sentence_text.append(data[data['Sentence #'] == s]['Word'].tolist())\n",
        "    sentence_post.append(data[data['Sentence #'] == s]['POS'].tolist())\n",
        "    sentence_ners.append(data[data['Sentence #'] == s]['Tag'].tolist())\n",
        "    #print(this_snt)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Wall time: 1min 12s\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQgqUDqTXKja",
        "colab_type": "text"
      },
      "source": [
        "### Caracteres por sentencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtaC58_aW9Kq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "charSet = set()\n",
        "for sentence in sentence_text:\n",
        "  #print(sentence)\n",
        "  for token in sentence:\n",
        "      for char1 in token:\n",
        "          charSet.add(char1)\n",
        "chartList = list(charSet)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ub8krQDvXddC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "80e0b0cb-5080-4a29-f3c4-2d4023eb9337",
        "tags": []
      },
      "source": [
        "print(len(chartList))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "113\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JpPq7s7XiQn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2Idx = {\"PAD\": 0, \"UNK\": 1}\n",
        "for c in chartList:\n",
        "    char2Idx[c] = len(char2Idx)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iajy66rGHZTW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fecdcdfd-0547-406b-f07c-1b2ee21a5674"
      },
      "source": [
        "len(char2Idx)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "115"
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZyIFtIPPYvQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len = 50"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmfd1c1FPm2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_len_char = 50"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utCvJEbaY1ky",
        "colab_type": "text"
      },
      "source": [
        "Se calculan los caraceteres por token en cada setentencia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuhXEPZ9YzRR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "cbc65285-7905-4176-f92f-72bb1aab9735",
        "tags": []
      },
      "source": [
        "%%time\n",
        "char_sentences = []\n",
        "for sent in sentence_text:\n",
        "  #print(sent)\n",
        "  char_sent =[]\n",
        "  for i in range(max_len):\n",
        "    char_token = []\n",
        "    for j in range(max_len_char):\n",
        "      #print(j)\n",
        "      try:\n",
        "        char = sent[i][j]\n",
        "\n",
        "        char_token.append(char2Idx.get(char))\n",
        "      except:\n",
        "        char_token.append(char2Idx.get('PAD'))\n",
        "        #print('PAD')\n",
        "    char_sent.append(char_token)\n",
        "  char_sentences.append(np.array(char_sent))\n",
        "      #if char in vocab_dict.keys()\n",
        "     \n",
        "  #for token in sent:\n",
        "    #print(token)\n",
        "  #  char_token = np.squeeze(index_sents(token, char2Idx))\n",
        "    #print(char_token)\n",
        "   # char_sent.append(char_token)\n",
        "  #char_sent = np.squeeze(char_sent)\n",
        "  #char_sentences.append(char_sent)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Wall time: 25.9 s\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Zv-t_Mao02",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " idx2Char = {v: k for k, v in char2Idx.items()}"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgksiwM6x1bt",
        "colab_type": "text"
      },
      "source": [
        "### Sentencias del conjunto de testeo\n",
        "#### Obtener vocabulario y entradas de índice\n",
        "\n",
        "necesitamos convertir la entrada de cadena a vectores enteros para la red de keras (la red pycrfsuite necesita cadenas, ya que extraerá vectores de características de las propias palabras).\n",
        "\n",
        "indexaremos cada palabra desde 1 de acuerdo con la frecuencia inversa (la palabra más común es 1, etc.) hasta el tamaño máximo de vocabulario. Reservaremos dos espacios, 0 para el índice PAD y MAX_VOCAB-1 para palabras fuera de vocabulario o desconocidas (OOV / UNK). Como esto es algo aburrido, lo puse en funciones externas. Los paquetes como keras y sklearn tienen herramientas más robustas para esto, pero una palabra simple: el diccionario índice funcionará bien para este experimento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lit0XVvFxfrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_VOCAB = len(set(words))+2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h15mjClavBaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bed6ff8d-bf07-46f1-8eec-c671ae364a70",
        "tags": []
      },
      "source": [
        "word2idx, idx2word = get_vocab(sentence_text, MAX_VOCAB-2)\n",
        "print(len(idx2word))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "43005\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CvqYZ8fxbPb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "17a3e127-b5a6-44a6-8638-82661c7e9c0b",
        "tags": []
      },
      "source": [
        "# POS and NER tag vocab dicts\n",
        "pos2idx, idx2pos = get_vocab(sentence_post, len(set(postags)))\n",
        "ner2idx, idx2ner = get_vocab(sentence_ners, len(set(nertags))+2)\n",
        "print(len(ner2idx))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "11\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zmDXohpxgI7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a3d3620-d89f-48a7-8f3e-eb2b21fb1572",
        "tags": []
      },
      "source": [
        "# index\n",
        "sentence_text_idx = index_sents(sentence_text, word2idx)\n",
        "sentence_post_idx = index_sents(sentence_post, pos2idx)\n",
        "sentence_ners_idx = index_sents(sentence_ners, ner2idx)\n",
        "#print(sentence_post_idx)\n",
        "print(len(sentence_post_idx))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "17345\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrtHUeWzyC6c",
        "colab_type": "text"
      },
      "source": [
        "### División de los conjuntos de prueba y entrenamiento.\n",
        "Dividimos los datos de entrenamiento en datos de entrenamiento y datos de prueba. los datos de prueba se usan solo para verificar el rendimiento del modelo. Un tercer conjunto, el conjunto de validación, puede separarse de nuestros datos de entrenamiento para el ajuste de hiperparámetros, aunque si utilizamos la validación cruzada k-fold, nuestro conjunto de validación cambiará cada vez."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZZ3F7W7yPjj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST_SIZE = 0.15"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EqX1abLxlNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices = [i for i in range(len(sentence_text))]\n",
        "#print(sentence_post_idx)\n",
        "\n",
        "#print(train_idx)\n",
        "#test_size=TEST_SIZE\n",
        "train_idx, test_idx, X_train_pos, X_test_pos, X_train_char, X_test_char = train_test_split(indices,sentence_post_idx, char_sentences ,test_size=TEST_SIZE)\n",
        "#X_train_pos,X_test_pos=train_test_split(indices, sentence_post_idx1 ,test_size=0.0001)\n",
        "\n",
        "\n",
        "def get_sublist(lst, indices):\n",
        "    result = []\n",
        "    for idx in indices:\n",
        "        result.append(lst[idx])\n",
        "    return result\n",
        "\n",
        "X_train_sents = get_sublist(sentence_text_idx, train_idx)\n",
        "X_test_sents = get_sublist(sentence_text_idx, test_idx)\n",
        "y_train_ner = get_sublist(sentence_ners_idx, train_idx)\n",
        "y_test_ner = get_sublist(sentence_ners_idx, test_idx)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrAh0vKQ_JdX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68fa81c6-448f-4594-9722-09c25ab93056"
      },
      "source": [
        "type(X_train_sents)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "list"
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPchOYpPgTM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "a0df2ea7-9acc-4f8e-cdd3-5edbb53df8a2"
      },
      "source": [
        "X_train_sents[0]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([   34,    18,   382,  2330,    17,     3,  1414, 13183,  4564,\n          29, 13641,    30,     2,     6, 13642,  8660,     5,    13,\n        9797,    12,   581,  6650,     2,    10,  1577, 29350,     8,\n       29351,    10,     2,     5,     6,  1777,   218,     1,  4304,\n           1,  1085,  1727,     7,    20,  2431,  5041,    27,  6594,\n           7,   571,     1,   223,    12,  7403, 29352,    12, 29353,\n           2,  8601,     7,    13, 13643,  1717,     9,   466,     1,\n        1778,     7,    28,  2083,     2,    82,    13,   621,  6823,\n          44,     9,   662,     4])"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELCVlexhFBm1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bc1d663-eaef-43a9-d8c6-be3c1af80d6a"
      },
      "source": [
        "type( np.asarray(X_train_char))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "numpy.ndarray"
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-jQoI--XRbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1db65aac-cb94-40ca-ca10-0dff7e709d6a"
      },
      "source": [
        "np.asarray(X_train_char).shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "(14743, 50, 50)"
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYvcUW2r-q6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train_char = sequence.pad_sequences(X_train_char, maxlen=30, truncating='post', padding='post')\n",
        "#X_test_char = sequence.pad_sequences(X_test_char, maxlen=30, truncating='post', padding='post')"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeorqWi8CVwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train_char_1[0]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbhhmgweycx9",
        "colab_type": "text"
      },
      "source": [
        "### Creamos word2vec embeddings para palabras, pos-tags\n",
        "Se ha demostrado que el uso de vectores de incrustación pre-entrenados para inicializar la capa de incrustación ayuda a la capacitación para diversas tareas de etiquetado de secuencias, como el etiquetado de POS (Huang, Xu & Yu 2015; Ma & Hovy 2016) y el Reconocimiento de entidades con nombre para inglés (Ma & Hovy 2016 ; Lee Changki 2017) y japonés (Misawa, Taniguchi, Miura y Ohkuma 2017).\n",
        "\n",
        "Como estamos usando las etiquetas POS como entrada secundaria, también entrenaremos un espacio de incrustación para estas. utilizaremos solo los datos de entrenamiento para crear las incrustaciones. Estoy usando Gensim para esta tarea, y estoy usando una función auxiliar para ajustar el Word2Vec que guarda la incrustación y también el diccionario de vocabulario. Se vectorizan 6185 sentencias y solo una de testeo que no se usa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5onx5uJyW9s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "f3ba3c9d-1b54-4dd5-8925-9031f0e5ad75"
      },
      "source": [
        "train_sent_texts = [sentence_text[idx] for idx in train_idx]\n",
        "        \n",
        "w2v_vocab, w2v_model = create_embeddings(train_sent_texts,\n",
        "                       embeddings_path='./embeddings/text_embeddings_acn.gensimmodel',\n",
        "                       vocab_path='./embeddings/text_mapping_anc.json',\n",
        "                       size=300,\n",
        "                       workers=4,\n",
        "                       iter=20)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xhk6NS-AzVAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "89f3a989-0388-4dd9-9eb3-62f720251170"
      },
      "source": [
        "\n",
        "# pos embeddings\n",
        "train_post_texts = [sentence_post[idx] for idx in train_idx]\n",
        "\n",
        "w2v_pvocab, w2v_pmodel = create_embeddings(train_post_texts,\n",
        "                         embeddings_path='./embeddings/pos_embeddings_anc.gensimmodel',\n",
        "                         vocab_path='./embeddings/pos_mapping_anc.json',\n",
        "                         size=300,\n",
        "                         workers=4,\n",
        "                         iter=20)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cKEOVjhzweD",
        "colab_type": "text"
      },
      "source": [
        "### save everything to numpy binaries for loading\n",
        "granted, pickle would probably be more suitable for a lot of these things. but over-reliance on numpy binaries is a bad habit i've picked up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-Gmw4kXzlBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def numpy_save(saves, names):\n",
        "    for idx, item in enumerate(saves):\n",
        "        np.save('./encoded/ancora/{0}.npy'.format(names[idx]), item)\n",
        "    return\n",
        "\n",
        "saves = [\n",
        "vocab,\n",
        "sentence_text_idx,\n",
        "sentence_post_idx,\n",
        "sentence_ners_idx,\n",
        "word2idx, idx2word,\n",
        "pos2idx, idx2pos,\n",
        "ner2idx, idx2ner,\n",
        "train_idx,\n",
        "test_idx,\n",
        "X_train_sents,\n",
        "X_test_sents,\n",
        "X_train_pos,\n",
        "X_test_pos,\n",
        "y_train_ner,\n",
        "y_test_ner,\n",
        "sentence_text,\n",
        "sentence_post,\n",
        "sentence_ners,\n",
        "idx2Char, char2Idx\n",
        "]\n",
        "\n",
        "names = [\n",
        "'vocab',\n",
        "'sentence_text_idx',\n",
        "'sentence_post_idx',\n",
        "'sentence_ners_idx',\n",
        "'word2idx', 'idx2word',\n",
        "'pos2idx', 'idx2pos',\n",
        "'ner2idx', 'idx2ner',\n",
        "'train_idx',\n",
        "'test_idx',\n",
        "'X_train_sents',\n",
        "'X_test_sents',\n",
        "'X_train_pos',\n",
        "'X_test_pos',\n",
        "'y_train_ner',\n",
        "'y_test_ner',\n",
        "'sentence_text',\n",
        "'sentence_post',\n",
        "'sentence_ners',\n",
        "'idx2char', 'char2idx']\n",
        "\n",
        "numpy_save(saves, names)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEan_Y0kHr5M",
        "colab_type": "text"
      },
      "source": [
        "### LOAD NPY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9tphirzz3q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_drive = './drive/My Drive/Colab Notebooks/' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10dJZS6L0KHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# network hyperparameters\n",
        "MAX_LENGTH = 50\n",
        "MAX_VOCAB = 43007    # see preprocessing.ipynb\n",
        "WORDEMBED_SIZE = 300 # see data_preprocessing.ipynb\n",
        "POS_EMBED_SIZE = 300 # see data_preprocessing.ipynb\n",
        "HIDDEN_SIZE = 400    # LSTM Nodes/Features/Dimension\n",
        "BATCH_SIZE = 64\n",
        "DROPOUTRATE = 0.25\n",
        "MAX_EPOCHS = 8       # max iterations, early stop condition below\n",
        "#print(y_test_ner)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K_9a3gi0id8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d348aa6f-c570-4d01-e1f5-ba14e30f8944"
      },
      "source": [
        "# load data from npys (see preprocessing.ipynb)\n",
        "print(\"loading data...\\n\")\n",
        "vocab = list(np.load(path_drive+'encoded/vocab.npy',allow_pickle=True))\n",
        "sentence_text = list(np.load(path_drive+'encoded/sentence_text.npy',allow_pickle=True))\n",
        "sentence_post = list(np.load(path_drive+'encoded/sentence_post.npy',allow_pickle=True))\n",
        "sentence_ners = list(np.load(path_drive+'encoded/sentence_ners.npy',allow_pickle=True))\n",
        "sentence_text_idx = np.load(path_drive+'encoded/sentence_text_idx.npy',allow_pickle=True)\n",
        "sentence_post_idx = np.load(path_drive+'encoded/sentence_post_idx.npy',allow_pickle=True)\n",
        "sentence_ners_idx = np.load(path_drive+'encoded/sentence_ners_idx.npy',allow_pickle=True)\n",
        "word2idx = np.load(path_drive+'encoded/word2idx.npy',allow_pickle=True).item()\n",
        "idx2word = np.load(path_drive+'encoded/idx2word.npy',allow_pickle=True).item()\n",
        "pos2idx = np.load(path_drive+'encoded/pos2idx.npy',allow_pickle=True).item()\n",
        "idx2pos = np.load(path_drive+'encoded/idx2pos.npy',allow_pickle=True).item()\n",
        "ner2idx = np.load(path_drive+'encoded/ner2idx.npy',allow_pickle=True).item()\n",
        "idx2ner = np.load(path_drive+'encoded/idx2ner.npy',allow_pickle=True).item()\n",
        "train_idx = np.load(path_drive+'encoded/train_idx.npy',allow_pickle=True)\n",
        "test_idx = np.load(path_drive+'encoded/test_idx.npy',allow_pickle=True)\n",
        "X_train_sents = np.load(path_drive+'encoded/X_train_sents.npy',allow_pickle=True)\n",
        "X_test_sents = np.load(path_drive+'encoded/X_test_sents.npy',allow_pickle=True)\n",
        "X_train_pos = np.load(path_drive+'encoded/X_train_pos.npy',allow_pickle=True)\n",
        "X_test_pos = np.load(path_drive+'encoded/X_test_pos.npy',allow_pickle=True)\n",
        "y_train_ner = np.load(path_drive+'encoded/y_train_ner.npy',allow_pickle=True)\n",
        "y_test_ner = np.load(path_drive+'encoded/y_test_ner.npy',allow_pickle=True)\n",
        "print(y_test_ner)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data...\n",
            "\n",
            "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 3, 1, 3, 1, 3, 1])\n",
            " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            " ...\n",
            " array([1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 4, 1, 1, 1, 1, 1, 1])\n",
            " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 6, 1])\n",
            " array([4, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 4,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tFv7F0m0uUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4bb31a90-bfe3-421c-d8ce-68c74d839c77"
      },
      "source": [
        "from embedding import load_vocab\n",
        "w2v_vocab, _ = load_vocab(path_drive+'Embeddings/text_mapping.json')\n",
        "w2v_model = Word2Vec.load(path_drive+'Embeddings/text_embeddings.gensimmodel')\n",
        "w2v_pvocab, _ = load_vocab(path_drive+'Embeddings/pos_mapping.json')\n",
        "w2v_pmodel = Word2Vec.load(path_drive+'Embeddings/pos_embeddings.gensimmodel')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eY0qYDvJkH_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train_char[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2_Ms3W084w",
        "colab_type": "text"
      },
      "source": [
        "### Secuencias de pad\n",
        "debemos 'rellenar' nuestras secuencias de entrada y salida a una longitud fija debido a la representación de gráfico fijo de Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi0I_mkB05gj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "ae5c31cc-169f-4ae1-a455-f3ddad289a29",
        "tags": []
      },
      "source": [
        "# zero-pad the sequences to max length\n",
        "print(\"zero-padding sequences...\\n\")\n",
        "X_train_sents = sequence.pad_sequences(X_train_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "X_test_sents = sequence.pad_sequences(X_test_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "X_train_pos = sequence.pad_sequences(X_train_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "X_test_pos = sequence.pad_sequences(X_test_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "y_train_ner = sequence.pad_sequences(y_train_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "y_test_ner = sequence.pad_sequences(y_test_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "\n",
        "print(len(X_train_sents))\n",
        "print(len(X_test_sents))\n",
        "print(len(X_train_pos))\n",
        "print(len(X_test_pos))\n",
        "print(len(y_train_ner))\n",
        "print(len(y_test_ner))\n",
        "\n",
        "#print(y_train_ner)\n",
        "print(y_test_ner)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "zero-padding sequences...\n\n14743\n2602\n14743\n2602\n14743\n2602\n[[1 4 1 ... 0 0 0]\n [1 1 1 ... 0 0 0]\n [1 1 1 ... 0 0 0]\n ...\n [1 1 1 ... 1 1 1]\n [1 1 1 ... 0 0 0]\n [1 1 1 ... 1 0 0]]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jc6K_AXpIoAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train_char[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62ou7eJ5J4t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#idx2Char[32]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3E9nke_1AK7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "278bf1fd-6ef7-420b-fa2a-16e5b9110313",
        "tags": []
      },
      "source": [
        "y_ner=y_test_ner\n",
        "print(y_ner)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[1 4 1 ... 0 0 0]\n [1 1 1 ... 0 0 0]\n [1 1 1 ... 0 0 0]\n ...\n [1 1 1 ... 1 1 1]\n [1 1 1 ... 0 0 0]\n [1 1 1 ... 1 0 0]]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqn3w_X51FFT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get the size of pos-tags, ner tags\n",
        "TAG_VOCAB = len(list(idx2pos.keys()))\n",
        "NER_VOCAB = len(list(idx2ner.keys()))\n",
        "CHAR_VOCAB = len(list(idx2Char.keys()))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n2hEOEO1Ifd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reshape data for CRF\n",
        "y_train_ner = y_train_ner[:, :, np.newaxis]\n",
        "y_test_ner = y_test_ner[:, :, np.newaxis]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIRsnzLe1Oo0",
        "colab_type": "text"
      },
      "source": [
        "### Precargar las incrustaciones pre-entrenadas\n",
        "Como se vio en estudios previos como Ma & Hovy 2016, se ha demostrado que cargar la capa de embeddings con vectores de embeddings preentrenados mejora el rendimiento de la red. Aquí inicializamos un embeddings en ceros y luego cargamos el embeddings desde el modelo previamente entrenado (si existe; puede que no se deba a los parámetros de Word2Vec)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Zsw0sR-1KvS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "acccc4cc-49e0-4842-f98f-5f356126fba6",
        "tags": []
      },
      "source": [
        "\n",
        "# create embedding matrices from custom pretrained word2vec embeddings\n",
        "word_embedding_matrix = np.zeros((MAX_VOCAB, WORDEMBED_SIZE))\n",
        "c = 0\n",
        "for word in word2idx.keys():\n",
        "    # get the word vector from the embedding model\n",
        "    # if it's there (check against vocab list)\n",
        "    if word in w2v_vocab:\n",
        "        c += 1\n",
        "        # get the word vector\n",
        "        word_vector = w2v_model[word]\n",
        "        # slot it in at the proper index\n",
        "        word_embedding_matrix[word2idx[word]] = word_vector\n",
        "print(\"adicionados\", c, \"vectores\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "adicionados8450vectores\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01G9493r1RT7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fb009650-993b-4003-b7e6-ae6d5d202d77",
        "tags": []
      },
      "source": [
        "pos_embedding_matrix = np.zeros((TAG_VOCAB, POS_EMBED_SIZE))\n",
        "c = 0\n",
        "for word in pos2idx.keys():\n",
        "    # get the word vector from the embedding model\n",
        "    # if it's there (check against vocab list)\n",
        "    if word in w2v_pvocab:\n",
        "        c += 1\n",
        "        # get the word vector\n",
        "        word_vector = w2v_pmodel[word]\n",
        "        # slot it in at the proper index\n",
        "        pos_embedding_matrix[pos2idx[word]] = word_vector\n",
        "print(\"adicionamos\", c, \"vectores\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "adicionamos14vectores\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUPvpb1c1T2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "c32bcaf1-e169-479e-c35b-dcfd0bf2e0fb",
        "tags": []
      },
      "source": [
        "\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 2263927790390038746\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 6661821563\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 8899923082343030611\nphysical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1\"\n]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBP62fNe6o64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.initializers import RandomUniform\n",
        "from keras.layers import TimeDistributed, Conv1D, Dense, Embedding, Input, Dropout, LSTM, Bidirectional, MaxPooling1D, \\\n",
        "    Flatten, concatenate"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wYSx4OvMGVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train_char[0]"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPmd6erbOkCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONV_SIZE = 3"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pALZ44GY1ak1",
        "colab_type": "text"
      },
      "source": [
        "# MODELO BILSTM_CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhRWNCqO1V6c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d5b9431f-0e5f-470f-9565-0750512aad9f"
      },
      "source": [
        "# define model\n",
        "\n",
        "# text layers : dense embedding > dropout > bi-LSTM\n",
        "#char_input = Input(shape=(MAX_LENGTH,),  name='char_input')\n",
        "char_input = Input(shape=(None, max_len_char),  name='char_input')\n",
        "char_embed = TimeDistributed(Embedding(input_dim=CHAR_VOCAB,output_dim=300, embeddings_initializer=RandomUniform(minval=-0.5, maxval=0.5), name=\"Character_embedding\"))(char_input)\n",
        "char_drpot = Dropout(DROPOUTRATE, name='char_dropout', trainable=True)(char_embed)\n",
        "\n",
        "conv1d_out = TimeDistributed(Conv1D(kernel_size=CONV_SIZE, filters=100, padding='same', activation='tanh', strides=1), name=\"Convolution\")(char_drpot)\n",
        "maxpool_out = TimeDistributed(MaxPooling1D(max_len_char), name=\"Maxpool\")(conv1d_out)\n",
        "char = TimeDistributed(Flatten(), name=\"Flatten\")(maxpool_out)\n",
        "char = Dropout(DROPOUTRATE)(char)\n",
        "\n",
        "txt_input = Input(shape=(None,), name='txt_input')\n",
        "txt_embed = Embedding(MAX_VOCAB, WORDEMBED_SIZE, #input_length=MAX_LENGTH,\n",
        "                      weights=[word_embedding_matrix],\n",
        "                      name='txt_embedding', trainable=True)(txt_input)\n",
        "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
        "\n",
        "# pos layers : dense embedding > dropout > bi-LSTM\n",
        "pos_input = Input(shape=(None,), name='pos_input')\n",
        "pos_embed = Embedding(TAG_VOCAB, POS_EMBED_SIZE, #input_length=MAX_LENGTH,\n",
        "                      weights=[pos_embedding_matrix],\n",
        "                      name='pos_embedding', trainable=True)(pos_input)\n",
        "pos_drpot = Dropout(DROPOUTRATE, name='pos_dropout')(pos_embed)\n",
        "\n",
        "# merged layers : merge (concat, average...) word and pos > bi-LSTM > bi-LSTM\n",
        "#mrg_cncat = concatenate([txt_drpot, pos_drpot, char_drpot], axis=2)\n",
        "mrg_cncat = concatenate([txt_drpot, pos_drpot, char])\n",
        "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
        "                          name='mrg_bidirectional_1')(mrg_cncat)\n",
        "\n",
        "# extra LSTM layer, if wanted\n",
        "mrg_drpot = Dropout(DROPOUTRATE, name='mrg_dropout')(mrg_lstml)\n",
        "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
        "                          name='mrg_bidirectional_2')(mrg_lstml)\n",
        "\n",
        "\n",
        "# final linear chain CRF layer\n",
        "crf = CRF(NER_VOCAB, sparse_target=True)\n",
        "mrg_chain = crf(mrg_lstml)\n",
        "\n",
        "model = Model(inputs=[txt_input, pos_input, char_input], outputs=mrg_chain)\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=crf.loss_function,\n",
        "              metrics=[crf.accuracy])"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf9E2DKn1h43",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "3a2b016d-5e11-43db-895f-45f126594569",
        "tags": []
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================================================\nchar_input (InputLayer)         (None, None, 50)     0\n__________________________________________________________________________________________________\ntime_distributed_1 (TimeDistrib (None, None, 50, 300 34500       char_input[0][0]\n__________________________________________________________________________________________________\nchar_dropout (Dropout)          (None, None, 50, 300 0           time_distributed_1[0][0]\n__________________________________________________________________________________________________\nConvolution (TimeDistributed)   (None, None, 50, 100 90100       char_dropout[0][0]\n__________________________________________________________________________________________________\ntxt_input (InputLayer)          (None, None)         0\n__________________________________________________________________________________________________\npos_input (InputLayer)          (None, None)         0\n__________________________________________________________________________________________________\nMaxpool (TimeDistributed)       (None, None, 1, 100) 0           Convolution[0][0]\n__________________________________________________________________________________________________\ntxt_embedding (Embedding)       (None, None, 300)    12902100    txt_input[0][0]\n__________________________________________________________________________________________________\npos_embedding (Embedding)       (None, None, 300)    4800        pos_input[0][0]\n__________________________________________________________________________________________________\nFlatten (TimeDistributed)       (None, None, 100)    0           Maxpool[0][0]\n__________________________________________________________________________________________________\ntxt_dropout (Dropout)           (None, None, 300)    0           txt_embedding[0][0]\n__________________________________________________________________________________________________\npos_dropout (Dropout)           (None, None, 300)    0           pos_embedding[0][0]\n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, None, 100)    0           Flatten[0][0]\n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, None, 700)    0           txt_dropout[0][0]\n                                                                 pos_dropout[0][0]\n                                                                 dropout_1[0][0]\n__________________________________________________________________________________________________\nmrg_bidirectional_1 (Bidirectio (None, None, 800)    3523200     concatenate_1[0][0]\n__________________________________________________________________________________________________\nmrg_bidirectional_2 (Bidirectio (None, None, 800)    3843200     mrg_bidirectional_1[0][0]\n__________________________________________________________________________________________________\ncrf_1 (CRF)                     (None, None, 11)     8954        mrg_bidirectional_2[0][0]\n==================================================================================================\nTotal params: 20,406,854\nTrainable params: 20,406,854\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag1vbh-41jCW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d08d3e5f-68d3-4012-a039-e8ad8547ae57"
      },
      "source": [
        "print(len(X_train_sents),len(X_train_pos),len(y_train_ner))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14743 14743 14743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXqNj8eRWkI4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7519e103-d085-466a-c4b4-27b234c31e2f"
      },
      "source": [
        "X_train_sents.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14743, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQbhlirKWmJ7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "721bc378-529f-4ec1-dad6-809e4e7fde82"
      },
      "source": [
        "X_train_pos.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14743, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxAHb--8WtF_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a342c02a-1f84-4e44-e70e-4020bc4edaac"
      },
      "source": [
        "type(X_train_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9jaP_-cWp1e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1106121a-6700-4948-a2c3-4617847e036e"
      },
      "source": [
        "type(X_train_char)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# lets assume `model` is main model \n",
        "model_json = model.to_json()\n",
        "with open(\"./models/model_in_json_ancora.json\", \"w\") as json_file:\n",
        "    json.dump(model_json, json_file)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5WgsWre1sWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "be6bc2e0-9f02-4ed8-c284-7ae046d03ff5"
      },
      "source": [
        "history = model.fit([X_train_sents, X_train_pos, np.asarray(X_train_char)], y_train_ner,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    epochs=10,\n",
        "\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "14743/14743 [==============================] - 123s 8ms/step - loss: 0.1246 - crf_viterbi_accuracy: 0.9625\n",
            "Epoch 2/10\n",
            "14743/14743 [==============================] - 115s 8ms/step - loss: 0.0374 - crf_viterbi_accuracy: 0.9886\n",
            "Epoch 3/10\n",
            "14743/14743 [==============================] - 112s 8ms/step - loss: 0.0214 - crf_viterbi_accuracy: 0.9932\n",
            "Epoch 4/10\n",
            "14743/14743 [==============================] - 114s 8ms/step - loss: 0.0102 - crf_viterbi_accuracy: 0.9958\n",
            "Epoch 5/10\n",
            "14743/14743 [==============================] - 113s 8ms/step - loss: 0.0022 - crf_viterbi_accuracy: 0.9974\n",
            "Epoch 6/10\n",
            "14743/14743 [==============================] - 114s 8ms/step - loss: -0.0037 - crf_viterbi_accuracy: 0.9983\n",
            "Epoch 7/10\n",
            "14743/14743 [==============================] - 112s 8ms/step - loss: -0.0090 - crf_viterbi_accuracy: 0.9989\n",
            "Epoch 8/10\n",
            "14743/14743 [==============================] - 114s 8ms/step - loss: -0.0137 - crf_viterbi_accuracy: 0.9992\n",
            "Epoch 9/10\n",
            "14743/14743 [==============================] - 114s 8ms/step - loss: -0.0183 - crf_viterbi_accuracy: 0.9994\n",
            "Epoch 10/10\n",
            "14743/14743 [==============================] - 113s 8ms/step - loss: -0.0228 - crf_viterbi_accuracy: 0.9996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRiEoDDOVF_w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15b26c3e-34cf-46f2-9afa-95a52915b2ae"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'crf_viterbi_accuracy'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg2JfhwfVAJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2S3H99_1whU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist_dict = history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAgHu9oD45UD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "539bb455-2379-440d-ca73-a5efeaa77c17"
      },
      "source": [
        "\n",
        "# save the model\n",
        "# because we are using keras-contrib, we must save weights like this, and load into network\n",
        "# (see decoding.ipynb)\n",
        "save_load_utils.save_all_weights(model, path_drive+'models/crf_model_char_ancora_10.h5')\n",
        "np.save(path_drive + 'models/hist_dict_char_ancora_10.npy', hist_dict)\n",
        "print(\"models saved!\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "models saved!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWAAMata5IKU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "2157be84-2edf-47b1-bf53-0509b13c31dc"
      },
      "source": [
        "print(len(X_test_sents),len(X_test_pos))\n",
        "print(X_test_sents)\n",
        "print(X_test_pos)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2602 2602\n",
            "[[ 1319   167    38 ...  1310    25    12]\n",
            " [42423    97     1 ...     0     0     0]\n",
            " [   23   648  1783 ...     0     0     0]\n",
            " ...\n",
            " [  410  3524    26 ...     0     0     0]\n",
            " [   10    85    71 ...     0     0     0]\n",
            " [10349     2  4169 ...   133     5     3]]\n",
            "[[10 13  1 ...  1 10  2]\n",
            " [ 5  8  2 ...  0  0  0]\n",
            " [ 3  1  7 ...  0  0  0]\n",
            " ...\n",
            " [ 3  1 10 ...  0  0  0]\n",
            " [ 4 15  9 ...  0  0  0]\n",
            " [ 9  4  5 ... 11 12  3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmwd14-N5K1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = model.predict([X_test_sents, X_test_pos, np.asarray(X_test_char)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBx0kLXq5Odc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a00c2485-3676-4393-f555-222aaabd1dc5"
      },
      "source": [
        "preds = np.argmax(preds, axis=-1)\n",
        "preds.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2602, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KJaNFfZ5Q6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "42489dca-69ef-434b-de17-f56005917ac3"
      },
      "source": [
        "trues = np.squeeze(y_test_ner, axis=-1)\n",
        "trues.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2602, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixA01rYC5TRr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_preds = [[idx2ner[t] for t in s] for s in preds]\n",
        "\n",
        "s_trues = [[idx2ner[t] for t in s] for s in trues]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJH38x-L5Xi6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "62234270-21e9-4f0b-fd7b-f2fa6c36babb"
      },
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "print(\"F1-score: {:.1%}\".format(f1_score(s_preds, s_trues)))\n",
        "print(\"Recall-score: {:.1%}\".format(recall_score(s_preds, s_trues)))\n",
        "print(\"Precision-score: {:.1%}\".format(precision_score(s_preds, s_trues)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.3.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "F1-score: 90.5%\n",
            "Recall-score: 90.1%\n",
            "Precision-score: 90.9%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p06pce7J5Zf6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e931f32e-aac9-4ebc-a5a7-59fbd63bb6a5"
      },
      "source": [
        "print(classification_report(s_trues, s_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           precision    recall  f1-score   support\n",
            "\n",
            "      PER       0.90      0.95      0.92      1405\n",
            "      PAD       1.00      1.00      1.00      2223\n",
            "      ORG       0.87      0.81      0.83      1539\n",
            "     MISC       0.88      0.89      0.88      1633\n",
            "      LOC       0.76      0.83      0.80       860\n",
            "\n",
            "micro avg       0.90      0.91      0.90      7660\n",
            "macro avg       0.90      0.91      0.90      7660\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stv8PwfF5gks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from itertools import chain\n",
        "def bio_classification_report(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    from scrapinghub's python-crfsuite example\n",
        "    \n",
        "    Classification report for a list of BIO-encoded sequences.\n",
        "    It computes token-level metrics and discards \"O\" labels.\n",
        "    \n",
        "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
        "    to calculate averages properly!\n",
        "    \"\"\"\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_) - {'O', 'PAD'}\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "    )\n",
        "    lb = LabelBinarizer()\n",
        "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
        "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
        "        \n",
        "    tagset = set(lb.classes_) - {'O'}\n",
        "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
        "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
        "    \n",
        "    return classification_report(\n",
        "        y_true_combined,\n",
        "        y_pred_combined,\n",
        "        labels = [class_indices[cls] for cls in tagset],\n",
        "        target_names = tagset,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhLDu95h5-Z1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "e9cfb21c-1e52-409c-e8ee-01ca2a2e7264"
      },
      "source": [
        "print(bio_classification_report(s_trues, s_preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       B-LOC       0.78      0.85      0.81       860\n",
            "       I-LOC       0.72      0.75      0.74       285\n",
            "      B-MISC       0.91      0.91      0.91      1633\n",
            "      I-MISC       0.88      0.74      0.80       833\n",
            "       B-ORG       0.91      0.83      0.87      1539\n",
            "       I-ORG       0.86      0.86      0.86       973\n",
            "       B-PER       0.91      0.96      0.93      1405\n",
            "       I-PER       0.94      0.95      0.95       918\n",
            "\n",
            "   micro avg       0.88      0.87      0.88      8446\n",
            "   macro avg       0.86      0.86      0.86      8446\n",
            "weighted avg       0.88      0.87      0.88      8446\n",
            " samples avg       0.06      0.06      0.06      8446\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY2pNstz6A18",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "ee122c08-400c-4420-af3b-f01af0c5a687"
      },
      "source": [
        "#print(y_ner)\n",
        "print(X_test_sents[:500])\n",
        "print(len(X_test_sents[:500]))\n",
        "decoded = []\n",
        "for sent_idx in range(len(X_test_sents[:500])):\n",
        "    \n",
        "    this_txt = sequence.pad_sequences([X_test_sents[sent_idx]], maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "    this_pos = sequence.pad_sequences([X_test_pos[sent_idx]], maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "    this_pred = model.predict([this_txt, this_pos])\n",
        "    this_pred = [np.argmax(p) for p in this_pred[0]]\n",
        "    np.shape(this_pred)\n",
        "    #print(this_pred)\n",
        "    # for each word in the sentence...\n",
        "    word, pos, tru, prd = [], [], [], []\n",
        "    for idx, wordid in enumerate(X_test_sents[sent_idx][:len(this_pred)]):\n",
        "        # decode word\n",
        "        word.append(idx2word[wordid])\n",
        "        # decode pos\n",
        "        #print(X_test_pos[sent_idx][idx])\n",
        "        pos.append(idx2pos[X_test_pos[sent_idx][idx]])\n",
        "        # decode true NER tag\n",
        "        #print(pos)\n",
        "        #print(y_ner[sent_idx][idx])\n",
        "        tru.append(idx2ner[y_ner[sent_idx][idx]])\n",
        "        # decode prediction\n",
        "        #print(tru)\n",
        "        prd.append(idx2ner[this_pred[idx]])\n",
        "        #print(prd)\n",
        "    \n",
        "    answ = pd.DataFrame(\n",
        "    {\n",
        "        'word': word,\n",
        "        'pos': pos,\n",
        "        'true': tru,\n",
        "        'pred': prd,\n",
        "        'skip' : [' ' for s in word]\n",
        "    })\n",
        "    answ = answ[['word', 'pos', 'true', 'pred', 'skip']]\n",
        "    answ = answ.T\n",
        "    decoded.append(answ)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1319   167    38 ...  1310    25    12]\n",
            " [42423    97     1 ...     0     0     0]\n",
            " [   23   648  1783 ...     0     0     0]\n",
            " ...\n",
            " [10505  1042     1 ...     3   302     9]\n",
            " [24137    17 16069 ...     0     0     0]\n",
            " [   10   822    69 ...     0     0     0]]\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBPXhFvM6LHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = pd.concat(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1BdXRfR6Nc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb687743-5c47-48a8-da56-e41b01652fef"
      },
      "source": [
        "print(result.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2500, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdJCw9LR6PJj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "80b6ef22-b920-4083-ffcb-f8c2c138d54b"
      },
      "source": [
        "result.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <td>Hace</td>\n",
              "      <td>cuatro</td>\n",
              "      <td>años</td>\n",
              "      <td>el</td>\n",
              "      <td>déficit</td>\n",
              "      <td>era</td>\n",
              "      <td>del</td>\n",
              "      <td>7</td>\n",
              "      <td>por</td>\n",
              "      <td>ciento</td>\n",
              "      <td>del</td>\n",
              "      <td>PIB</td>\n",
              "      <td>y</td>\n",
              "      <td>la</td>\n",
              "      <td>Seguridad</td>\n",
              "      <td>Social</td>\n",
              "      <td>estaba</td>\n",
              "      <td>\"</td>\n",
              "      <td>en</td>\n",
              "      <td>quiebra</td>\n",
              "      <td>\"</td>\n",
              "      <td>y</td>\n",
              "      <td>hoy</td>\n",
              "      <td>,</td>\n",
              "      <td>recordó</td>\n",
              "      <td>,</td>\n",
              "      <td>el</td>\n",
              "      <td>déficit</td>\n",
              "      <td>es</td>\n",
              "      <td>del</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pos</th>\n",
              "      <td>AUX</td>\n",
              "      <td>NUM</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>AUX</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NUM</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NUM</td>\n",
              "      <td>ADP</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>DET</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>AUX</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>ADP</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>ADV</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>VERB</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>DET</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>AUX</td>\n",
              "      <td>ADP</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>true</th>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>I-MISC</td>\n",
              "      <td>I-MISC</td>\n",
              "      <td>O</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-ORG</td>\n",
              "      <td>I-ORG</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pred</th>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>I-MISC</td>\n",
              "      <td>I-MISC</td>\n",
              "      <td>O</td>\n",
              "      <td>B-MISC</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>B-ORG</td>\n",
              "      <td>I-ORG</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>skip</th>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        0       1     2    3        4   ...     25   26       27   28   29\n",
              "word  Hace  cuatro  años   el  déficit  ...      ,   el  déficit   es  del\n",
              "pos    AUX     NUM  NOUN  DET     NOUN  ...  PUNCT  DET     NOUN  AUX  ADP\n",
              "true     O       O     O    O        O  ...      O    O        O    O    O\n",
              "pred     O       O     O    O        O  ...      O    O        O    O    O\n",
              "skip                                    ...                               \n",
              "\n",
              "[5 rows x 30 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQhxVRUMlzQs",
        "colab_type": "text"
      },
      "source": [
        "## Etiquetado de texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32_V8hoz-b2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tag import StanfordPOSTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S1tN2gQ-fhO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "outputId": "2375d51c-e34c-4235-cb1e-5ecdac2ab2da"
      },
      "source": [
        "!wget 'https://nlp.stanford.edu/software/stanford-tagger-4.0.0.zip'\n",
        "!unzip stanford-tagger-4.0.0.zip\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-13 22:34:50--  https://nlp.stanford.edu/software/stanford-tagger-4.0.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 78260695 (75M) [application/zip]\n",
            "Saving to: ‘stanford-tagger-4.0.0.zip’\n",
            "\n",
            "stanford-tagger-4.0 100%[===================>]  74.63M  57.5MB/s    in 1.3s    \n",
            "\n",
            "2020-07-13 22:34:51 (57.5 MB/s) - ‘stanford-tagger-4.0.0.zip’ saved [78260695/78260695]\n",
            "\n",
            "Archive:  stanford-tagger-4.0.0.zip\n",
            "   creating: stanford-tagger-4.0.0/\n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger-4.0.0-javadoc.jar  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger.bat  \n",
            "  inflating: stanford-tagger-4.0.0/sample-input.txt  \n",
            "  inflating: stanford-tagger-4.0.0/sample-output.txt  \n",
            "   creating: stanford-tagger-4.0.0/data/\n",
            "  inflating: stanford-tagger-4.0.0/data/enclitic-inflections.data  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger-gui.sh  \n",
            "  inflating: stanford-tagger-4.0.0/TaggerDemo.java  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger-4.0.0.jar  \n",
            "   creating: stanford-tagger-4.0.0/models/\n",
            "  inflating: stanford-tagger-4.0.0/models/arabic-train.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/arabic.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/chinese-nodistsim.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/README-Models.txt  \n",
            "  inflating: stanford-tagger-4.0.0/models/german-ud.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-caseless-left3words-distsim.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/chinese-distsim.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-caseless-left3words-distsim.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/arabic.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/german-ud.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/french-ud.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-left3words-distsim.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/french-ud.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/arabic-train.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/spanish-ud.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/english-bidirectional-distsim.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/models/chinese-distsim.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/spanish-ud.tagger.props  \n",
            "  inflating: stanford-tagger-4.0.0/models/chinese-nodistsim.tagger  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger.sh  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger-gui.bat  \n",
            "  inflating: stanford-tagger-4.0.0/README.txt  \n",
            "  inflating: stanford-tagger-4.0.0/build.xml  \n",
            "  inflating: stanford-tagger-4.0.0/TaggerDemo2.java  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger-4.0.0-sources.jar  \n",
            "  inflating: stanford-tagger-4.0.0/LICENSE.txt  \n",
            "  inflating: stanford-tagger-4.0.0/stanford-postagger.jar  \n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdIRvz5w61SO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0b747616-9703-4a0b-e854-55718aae1d8c"
      },
      "source": [
        "st = StanfordPOSTagger('/content/stanford-tagger-4.0.0/models/spanish-ud.tagger','/content/stanford-tagger-4.0.0/stanford-postagger.jar')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/tag/stanford.py:149: DeprecationWarning: \n",
            "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
            "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
            "  super(StanfordPOSTagger, self).__init__(*args, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pdbLtF-AyNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = '''Para ser dos países con una historia, circunstancias y geografía tan parecidas, Canadá y Estados Unidos han terminado teniendo una identidad nacional bien distinta.\n",
        "\n",
        "Los dos gigantes norteamericanos comparten un continente, 8.893 kilómetros de frontera terrestre -la más larga del mundo- y una historia llena de paralelos y similitudes.\n",
        "\n",
        "Ambos empezaron como naciones fundadas por europeos en terrenos arrebatados a los indígenas pero eventualmente, gracias a la inmigración global, se convirtieron en el último siglo en sociedades enormemente multirraciales.'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBKqufPMOuq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRkDN17YOz3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_sentences = sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6ZeqEmgBTjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postag = [st.tag(s.split()) for s in sample_sentences ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1pdrOrLBVbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_sample = []\n",
        "pos_sample = []\n",
        "for input in postag:\n",
        "  tmp_words = []\n",
        "  tmp_pos = []\n",
        "  for word, pos in input:\n",
        "    tmp_words.append(word)\n",
        "    tmp_pos.append(pos)\n",
        "  word_sample.append(tmp_words)\n",
        "  pos_sample.append(tmp_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JS9pro_UmIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def GetCharSentence(sentences):\n",
        "  char_sentences = []\n",
        "  for sent in sentences:\n",
        "    char_sent = []\n",
        "    for i in range(MAX_LENGTH):\n",
        "      char_token = []\n",
        "      for j in range(max_len_char):\n",
        "        #print(j)\n",
        "        try:\n",
        "          char = sent[i][j]\n",
        "\n",
        "          char_token.append(char2Idx.get(char))\n",
        "        except:\n",
        "          char_token.append(char2Idx.get('PAD'))\n",
        "          #print('PAD')\n",
        "      char_sent.append(char_token)\n",
        "    char_sentences.append(np.array(char_sent))\n",
        "  return char_sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ38adAfU0Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_test = GetCharSentence(word_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ7qD9P2YXjM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "bc7361a8-68b9-4d01-8716-f5df66f4110f"
      },
      "source": [
        "char_test[0][0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 66,  67, 107,  67,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ngMG6eYe2q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92229b66-b51b-4132-c35b-5ec8963e8c5d"
      },
      "source": [
        "idx2Char[107]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "'r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIdi5wdFKg0H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "b8442ef4-0d15-439a-f2a5-7757d795ac6d"
      },
      "source": [
        "print(word_sample)\n",
        "print(pos_sample)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Para', 'ser', 'dos', 'países', 'con', 'una', 'historia,', 'circunstancias', 'y', 'geografía', 'tan', 'parecidas,', 'Canadá', 'y', 'Estados', 'Unidos', 'han', 'terminado', 'teniendo', 'una', 'identidad', 'nacional', 'bien', 'distinta.'], ['Los', 'dos', 'gigantes', 'norteamericanos', 'comparten', 'un', 'continente,', '8.893', 'kilómetros', 'de', 'frontera', 'terrestre', '-la', 'más', 'larga', 'del', 'mundo-', 'y', 'una', 'historia', 'llena', 'de', 'paralelos', 'y', 'similitudes.'], ['Ambos', 'empezaron', 'como', 'naciones', 'fundadas', 'por', 'europeos', 'en', 'terrenos', 'arrebatados', 'a', 'los', 'indígenas', 'pero', 'eventualmente,', 'gracias', 'a', 'la', 'inmigración', 'global,', 'se', 'convirtieron', 'en', 'el', 'último', 'siglo', 'en', 'sociedades', 'enormemente', 'multirraciales.']]\n",
            "[['ADP', 'AUX', 'NUM', 'NOUN', 'ADP', 'DET', 'NUM', 'NOUN', 'CCONJ', 'NOUN', 'ADV', 'PROPN', 'PROPN', 'CCONJ', 'PROPN', 'PROPN', 'AUX', 'VERB', 'AUX', 'DET', 'NOUN', 'ADJ', 'ADV', 'PROPN'], ['DET', 'NUM', 'NOUN', 'ADJ', 'VERB', 'DET', 'NOUN', 'NUM', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'NUM', 'ADV', 'ADJ', 'ADP', 'PROPN', 'CCONJ', 'DET', 'NOUN', 'ADJ', 'ADP', 'ADJ', 'CCONJ', 'PROPN'], ['NUM', 'AUX', 'SCONJ', 'NOUN', 'ADJ', 'ADP', 'NOUN', 'ADP', 'NOUN', 'ADJ', 'ADP', 'DET', 'NOUN', 'CCONJ', 'NUM', 'NOUN', 'ADP', 'DET', 'NOUN', 'PROPN', 'PRON', 'VERB', 'ADP', 'DET', 'ADJ', 'NOUN', 'ADP', 'NOUN', 'ADV', 'ADJ']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y51Xa1enBqFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_sample_idx = index_sents(word_sample, word2idx)\n",
        "post_sampple_idx = index_sents(pos_sample, pos2idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9D1FKDyvIolC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_sample_sents = sequence.pad_sequences(sentences_sample_idx, maxlen=MAX_LENGTH, truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qXvYT57Mve_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "35189eb4-c1b5-457a-b88b-1799dc8f51b2"
      },
      "source": [
        "X_sample_sents[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   42,    36,  8704,  4366,  5923,    13, 43004, 43004,   584,\n",
              "           1,  1258, 10075, 43004,    31,  1910,    12, 43004,     8,\n",
              "          18,   336,  2563,     1,  8845,     8, 43004,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtFgKk3WJElg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_sampple_idx = sequence.pad_sequences(post_sampple_idx, maxlen=MAX_LENGTH, truncating='post', padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suY20nURDbLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict([X_sample_sents,post_sampple_idx, np.asarray(char_test)])\n",
        "#print(predictions, predictions.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8ONXUDXM89i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2v5gyaMGcu8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(predictions, axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O_Z-W4VNTu7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "5a923a0e-4184-4b43-8cd7-95f63eb7fc75"
      },
      "source": [
        "predictions"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 7, 9, 1, 7, 9, 1, 1, 1, 1, 1, 1,\n",
              "        1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 7, 1, 1, 1, 1, 1,\n",
              "        1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0],\n",
              "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 4, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EC8dW22DGwwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_predictions = [[idx2ner[t] for t in s] for s in predictions]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvTscbdGD3Ph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "855edff0-a528-4763-83e9-d8a1b08349b6"
      },
      "source": [
        "#!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "for idx,s in enumerate(sample_sentences):\n",
        "  heads1 = s.split()\n",
        "  body1 = [s_predictions[idx][:len(s.split())]]\n",
        "  print(tabulate(body1, headers=heads1))\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Para    ser    dos    países    con    una    historia,    circunstancias    y    geografía    tan    parecidas,    Canadá    y    Estados    Unidos    han    terminado    teniendo    una    identidad    nacional    bien    distinta.\n",
            "------  -----  -----  --------  -----  -----  -----------  ----------------  ---  -----------  -----  ------------  --------  ---  ---------  --------  -----  -----------  ----------  -----  -----------  ----------  ------  -----------\n",
            "O       O      O      O         O      O      O            O                 O    O            O      B-LOC         I-LOC     O    B-LOC      I-LOC     O      O            O           O      O            O           O       B-PER\n",
            "\n",
            "\n",
            "Los    dos    gigantes    norteamericanos    comparten    un    continente,    8.893    kilómetros    de    frontera    terrestre    -la     más    larga    del    mundo-    y    una    historia    llena    de    paralelos    y    similitudes.\n",
            "-----  -----  ----------  -----------------  -----------  ----  -------------  -------  ------------  ----  ----------  -----------  ------  -----  -------  -----  --------  ---  -----  ----------  -------  ----  -----------  ---  --------------\n",
            "O      O      O           O                  O            O     O              B-MISC   O             O     O           O            B-MISC  O      O        O      B-LOC     O    O      O           O        O     O            O    B-PER\n",
            "\n",
            "\n",
            "Ambos    empezaron    como    naciones    fundadas    por    europeos    en    terrenos    arrebatados    a    los    indígenas    pero    eventualmente,    gracias    a    la    inmigración    global,    se    convirtieron    en    el    último    siglo    en    sociedades    enormemente    multirraciales.\n",
            "-------  -----------  ------  ----------  ----------  -----  ----------  ----  ----------  -------------  ---  -----  -----------  ------  ----------------  ---------  ---  ----  -------------  ---------  ----  --------------  ----  ----  --------  -------  ----  ------------  -------------  -----------------\n",
            "O        O            O       O           O           O      O           O     O           O              O    O      O            O       B-MISC            O          O    O     O              B-PER      O     O               O     O     O         O        O     O             O              O\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6ijkY2hEXkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoY0FN53FJqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbFAN-77JP_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}